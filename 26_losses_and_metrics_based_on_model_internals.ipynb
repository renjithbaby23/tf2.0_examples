{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom losses and metrics we defined earlier were all based on the labels and the\n",
    "predictions (and optionally sample weights). However, you will occasionally want to\n",
    "define losses based on other parts of your model, such as the weights or activations of\n",
    "its hidden layers. This may be useful for regularization purposes, or to monitor some\n",
    "internal aspect of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a custom loss based on model internals, just compute it based on any part\n",
    "of the model you want, then pass the result to the ```add_loss()``` method. \n",
    "\n",
    "For example, the following custom model represents a standard MLP regressor with 5 hidden \n",
    "layers, except it also implements a reconstruction loss (we add an extra Dense\n",
    "layer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\n",
    "the model. Since the reconstruction must have the same shape as the model’s inputs,\n",
    "we need to create this Dense layer in the ```build()``` method to have access to the shape\n",
    "of the inputs. In the ```call()``` method, we compute both the regular output of the MLP,\n",
    "plus the output of the reconstruction layer. We then compute the mean squared difference \n",
    "between the reconstructions and the inputs, and we add this value (times 0.05) to the \n",
    "model’s list of losses by calling ```add_loss()```. \n",
    "\n",
    "During training, tf.keras will add this loss to the main loss (which is why we \n",
    "scaled down the reconstruction loss,to ensure the main loss dominates). \n",
    "As a result, the model will be forced to preserve as much information as possible \n",
    "through the hidden layers, even information that is not directly useful for the regression\n",
    "task itself. In practice, this loss sometimes improves generalization; \n",
    "it is a regularization loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(tf.keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * reconstruction_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)\n",
    "\n",
    "model = ReconstructingRegressor(1)\n",
    "# model.build(tf.TensorShape([None, 8]))       # <= Fails if this line is removed\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now during the training, it will show both 'mse' loss and reconstruction_loss\n",
    "```\n",
    "history = model.fit(X, y, epochs=2)\n",
    "\n",
    "Epoch 1/5\n",
    "11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\n",
    "Epoch 2/5\n",
    "11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n",
    "[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
