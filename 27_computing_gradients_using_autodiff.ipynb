{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial derivative wrt w1, [(w1, w2) = (5, 3)]:  36.000003007075065\n"
     ]
    }
   ],
   "source": [
    "# Computing gradients using epsilon\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "\n",
    "print(\"partial derivative wrt w1, [(w1, w2) = (5, 3)]: \", (f(w1 + eps, w2) - f(w1, w2)) / eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial derivative wrt w2, [(w1, w2) = (5, 3)]:  10.000000003174137\n"
     ]
    }
   ],
   "source": [
    "print(\"partial derivative wrt w2, [(w1, w2) = (5, 3)]: \", (f(w1, w2 + eps) - f(w1, w2)) / eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw1 : 36.0 \t dw2 : 10.0 \n"
     ]
    }
   ],
   "source": [
    "# Using GradientTape\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "\n",
    "print(\"dw1 : {} \\t dw2 : {} \".format(gradients[0].numpy(), gradients[1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only is the result accurate (the precision is only limited by the floating\n",
    "point errors), but the ```gradient()``` method only goes through the recorded computations \n",
    "once (in reverse order), no matter how many variables there are, so it is \n",
    "incredibly efficient. It’s like magic!\n",
    "\n",
    "Only put the strict minimum inside the ```tf.GradientTape()``` block,\n",
    "to save memory. Alternatively, you can pause recording by creating\n",
    "a ```with tape.stop_recording()``` block inside the ```tf.GradientTape()``` block.\n",
    "\n",
    "**The tape is automatically erased immediately after you call its gradient() method, so\n",
    "you will get an exception if you try to call gradient() twice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_dw1 : tf.Tensor(36.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GradientTape.gradient can only be called once on non-persistent tapes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5586b92c43c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdz_dw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# => tensor 36.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dz_dw1 :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdz_dw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdz_dw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# RuntimeError!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dz_dw2: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdz_dw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \"\"\"\n\u001b[1;32m    952\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m       raise RuntimeError(\"GradientTape.gradient can only be called once on \"\n\u001b[0m\u001b[1;32m    954\u001b[0m                          \"non-persistent tapes.\")\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GradientTape.gradient can only be called once on non-persistent tapes."
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\n",
    "print(\"dz_dw1 :\", dz_dw1)\n",
    "dz_dw2 = tape.gradient(z, w2) # RuntimeError!\n",
    "print(\"dz_dw2: \", dz_dw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you need to call ```gradient()``` more than once, you must make the tape persistent,\n",
    "and delete it when you are done with it to free resources:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_dw1 : tf.Tensor(36.0, shape=(), dtype=float32)\n",
      "dz_dw2:  tf.Tensor(36.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\n",
    "print(\"dz_dw1 :\", dz_dw1)\n",
    "dz_dw2 = tape.gradient(z, w2) # RuntimeError!\n",
    "print(\"dz_dw2: \", dz_dw1)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By default, the tape will only track operations involving variables, so if you try to\n",
    "compute the gradient of z with regards to anything else than a variable, the result will\n",
    "be None.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can force the tape to watch any tensors you like, to record every operation \n",
    "that involves them. You can then compute gradients with regards to these tensors, \n",
    "as if they were variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "gradients = tape.gradient(z, [c1, c2]) # returns [None, None]\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc1 : 36.0 \t dc2 : 10.0 \n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "print(\"dc1 : {} \\t dc2 : {} \".format(gradients[0].numpy(), gradients[1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful in some cases, for example if you want to implement a regulariza‐\n",
    "tion loss that penalizes activations that vary a lot when the inputs vary little: the loss\n",
    "will be based on the gradient of the activations with regards to the inputs. Since the\n",
    "inputs are not variables, you would need to tell the tape to watch them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In some rare cases you may want to stop gradients from backpropagating through\n",
    "some part of your neural network. To do this, you must use the ```tf.stop_gradient()``` \n",
    "function: it just returns its inputs during the forward pass (like ```tf.identity()```), \n",
    "but it does not let gradients through during backpropagation (it acts like a constant).**\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=1083, shape=(), dtype=float32, numpy=30.0>, None]\n"
     ]
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2) # same result as without stop_gradient()\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]\n",
    "\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you may occasionally run into some numerical issues when computing gradients. \n",
    "For example, if you compute the gradients of the my_softplus() function for\n",
    "large inputs, the result will be NaN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because computing the gradients of this function using autodiff leads to some\n",
    "numerical difficulties: due to floating point precision errors, autodiff ends up computing infinity divided by infinity (which returns NaN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can analytically find that the derivative of the softplus \n",
    "function is just $ 1 / (1 + 1 / exp(x))$, which is numerically stable. \n",
    "Next, we can tell TensorFlow to use this stable function when\n",
    "computing the gradients of the my_softplus() function, by decorating it with\n",
    "``` @tf.custom_gradient ``` , and making it return both its normal output and the function\n",
    "that computes the derivatives (note that it will receive as input the gradients that were\n",
    "backpropagated so far, down to the softplus function, and according to the chain rule\n",
    "we should multiply them with this function’s gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
