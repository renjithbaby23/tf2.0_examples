{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a custom activation function (equivalent to keras.activations.softplus or tf.nn.softplus )\n",
    "\n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a custom Glorot initializer (equivalent to keras.initializers.glorot_normal )\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a custom l1 regularizer (equivalent to keras.regularizers.l1(0.01) ) \n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a custom constraint that ensures weights are all positive (equivalent to keras.constraints.nonneg() or tf.nn.relu)\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the arguments depend on the type of custom function. These custom\n",
    "functions can then be used normally, for example:\n",
    "```\n",
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                            kernel_initializer=my_glorot_initializer,\n",
    "                            kernel_regularizer=my_l1_regularizer,\n",
    "                            kernel_constraint=my_positive_weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **The activation function will be applied to the output of this Dense layer, and its result will be passed on to the next layer.**\n",
    "2. **The layer’s weights will be initialized using the value returned by the initializer.** \n",
    "3. **At each training step the weights will be passed to the regularization function to compute the regularization loss, which will be added to the main loss to get the final loss used for training.**\n",
    "4. **Finally, the constraint function will be called after each training step, and the layer’s weights will be replaced by the con‐strained weights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a function has some hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate class, such as keras.regulariz\n",
    "ers.Regularizer , keras.constraints.Constraint , keras.initializers.Initializer or keras.layers.Layer (for any layer, including activation functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
