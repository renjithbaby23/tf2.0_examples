{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some rare cases, the ```fit()``` method may not be flexible enough for what you need\n",
    "to do. For example, **the Wide and Deep paper we discussed ago** actually uses two different optimizers: one for the wide path and the other for the deep path.\n",
    "Since the ```fit()``` method only uses one optimizer (the one that we specify when compiling the model), implementing this paper requires writing your own custom\n",
    "loop.\n",
    "\n",
    "You may also like to write your own custom training loops simply to feel more \n",
    "confident that it does precisely what you intent it to do (perhaps you are unsure about\n",
    "some details of the ```fit()``` method). It can sometimes feel safer to make everything\n",
    "explicit. However, remember that writing a custom training loop will make your code\n",
    "longer, more error prone and harder to maintain.\n",
    "\n",
    "\n",
    "Unless you really need the extra flexibility, you should prefer using\n",
    "the ```fit()``` method rather than implementing your own training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import time\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the sklearn california housing data\n",
    "housing = datasets.fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = model_selection.train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "# Standard scaling the data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First build a simple model. No need to compile it since we are going to handle the training loop manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = tf.keras.regularizers.l2(0.05)\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(30, \n",
    "                                                          activation=\"elu\", \n",
    "                                                          kernel_initializer=\"he_normal\", \n",
    "                                                          kernel_regularizer=l2_reg),\n",
    "                                    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random batch generation\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display the training status\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "    for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "    end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 - loss: 0.0900 - mean_square: 858.5000\n"
     ]
    }
   ],
   "source": [
    "mean_loss = tf.keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = tf.keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fancier version\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3500/10000 [=>....]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress_bar(3500, 10000, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - loss: 0.0900 - mean_square: 858.5000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is self-explanatory, unless you are unfamiliar with Python string format‐\n",
    "ting: {:.4f} will format a float with 4 digits after the decimal point. Moreover, using\n",
    "\\r (carriage return) along with end=\"\" ensures that the status bar always gets printed\n",
    "on the same line. In the notebook, the print_status_bar() function also includes a\n",
    "progress bar, but you could use the handy tqdm library instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train_scaled) // batch_size\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 [==============================] - mean: 1.2960 - mean_absolute_error: 0.8989\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - mean: 1.3297 - mean_absolute_error: 0.9072\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - mean: 1.2754 - mean_absolute_error: 0.8905\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - mean: 1.2982 - mean_absolute_error: 0.8994\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - mean: 1.3096 - mean_absolute_error: 0.9008\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    # print(\"*\"*10)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    # print(\"*\"*10)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• We create two nested loops: one for the epochs, the other for the batches within\n",
    "an epoch.\n",
    "\n",
    "• Then we sample a random batch from the training set.\n",
    "\n",
    "• Inside the tf.GradientTape() block, we make a prediction for one batch (using\n",
    "the model as a function), and we compute the loss: it is equal to the main loss\n",
    "plus the other losses (in this model, there is one regularization loss per layer).\n",
    "Since the mean_squared_error() function returns one loss per instance, we\n",
    "compute the mean over the batch using tf.reduce_mean() (if you wanted to\n",
    "apply different weights to each instance, this is where you would do it). The \n",
    "regularization losses are already reduced to a single scalar each, so we just need to\n",
    "sum them (using tf.add_n() , which sums multiple tensors of the same shape\n",
    "and data type).\n",
    "\n",
    "• Next, we ask the tape to compute the gradient of the loss with regards to each\n",
    "trainable variable (not all variables!), and we apply them to the optimizer to \n",
    "perform a Gradient Descent step.\n",
    "\n",
    "• Next we update the mean loss and the metrics (over the current epoch), and we\n",
    "display the status bar.\n",
    "\n",
    "• At the end of each epoch, we display the status bar again to make it look \n",
    "complete 11 and to print a line feed, and we reset the states of the mean loss and the\n",
    "metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, this training loop does not handle layers that behave differently\n",
    "during training and testing (e.g., BatchNormalization or Dropout ). To handle these,\n",
    "you need to call the model with training=True and make sure it propagates this to\n",
    "every layer that needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
