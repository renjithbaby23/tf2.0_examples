{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may occasionally want to build an architecture that contains an exotic layer for\n",
    "which TensorFlow does not provide a default implementation. In this case, you will\n",
    "need to create a custom layer. Or sometimes you may simply want to build a very\n",
    "repetitive architecture, containing identical blocks of layers repeated many times, and\n",
    "it would be convenient to treat each block of layers as a single layer. For example, if\n",
    "the model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\n",
    "define a custom layer D containing layers A, B, C, and your model would then simply\n",
    "be D, D, D. Let’s see how to build custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Custom layers with single input and single output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some layers have no weights, such as ```tf.keras.layers.Flatten``` or ```tf.keras.layers.ReLU``` . \n",
    "If you want to create a custom layer without any weights, the simplest\n",
    "option is to write a function and wrap it in a keras.layers.Lambda layer. For example, \n",
    "the following layer will apply the exponential function to its inputs:\n",
    "```\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "```\n",
    "\n",
    "**This custom layer can then be used like any other layer, using the sequential API, the\n",
    "functional API, or the subclassing API.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a custom stateful layer (i.e., a layer with weights), you need to create a subclass of the keras.layers.Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name=\"kernel\", \n",
    "                                      shape=[batch_input_shape[-1], self.units], \n",
    "                                      initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(name=\"bias\", \n",
    "                                    shape=[self.units], \n",
    "                                    initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config,\n",
    "                \"units\": self.units,\n",
    "                \"activation\": tf.keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walk through this code:\n",
    "\n",
    "• The constructor takes all the hyperparameters as arguments (in this example just\n",
    "units and activation ), and importantly it also takes a ```**kwargs``` argument. It\n",
    "calls the parent constructor, passing it the kwargs : this takes care of standard\n",
    "arguments such as ```input_shape``` , ```trainable``` , ```name``` , and so on. Then it saves the\n",
    "hyperparameters as attributes, converting the activation argument to the\n",
    "appropriate activation function using the tf.keras.activations.get() function (it\n",
    "accepts functions, standard strings like \"relu\" or \"selu\" , or simply None).\n",
    "\n",
    "\n",
    "• **The build() method’s role is to create the layer’s variables, by calling the\n",
    "add_weight() method for each weight. The build() method is called the first\n",
    "time the layer is used**. At that point, tf.keras will know the shape of this layer’s\n",
    "inputs, and it will pass it to the ```build()``` method, which is often necessary to create \n",
    "some of the weights. For example, we need to know the number of neurons in\n",
    "the previous layer in order to create the connection weights matrix (i.e., the \"kernel\"): \n",
    "this corresponds to the size of the last dimension of the inputs. At the end\n",
    "of the build() method (and only at the end), you must call the parent’s build()\n",
    "method: this tells tf.keras that the layer is built (it just sets self.built = True).\n",
    "\n",
    "\n",
    "• The ```call()``` method actually performs the desired operations. In this case, we\n",
    "compute the matrix multiplication of the inputs X and the layer’s kernel, we add\n",
    "the bias vector, we apply the activation function to the result, and this gives us the\n",
    "output of the layer.\n",
    "\n",
    "\n",
    "• The ```compute_output_shape()``` method simply returns the shape of this layer’s\n",
    "outputs. In this case, it is the same shape as the inputs, except the last dimension\n",
    "is replaced with the number of neurons in the layer. Note that in tf.keras, shapes\n",
    "are instances of the tf.TensorShape class, which you can convert to Python lists\n",
    "using as_list(). \n",
    "\n",
    "You can generally omit the compute_output_shape() method, as tf.keras automatically \n",
    "infers the output shape, except when the layer is dynamic. \n",
    "In other Keras implementations, this method is either required or by default it assumes \n",
    "the output shape is the same as the input shape.\n",
    "\n",
    "\n",
    "• The ```get_config()``` method is just like earlier. Note that **we save the activation\n",
    "function’s full configuration by calling tf.keras.activations.serialize().**\n",
    "\n",
    "You can now use a MyDense layer just like any other layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Custom Layers with multiple inputs and/or outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a layer with multiple inputs (e.g., Concatenate ), the argument to the ```call()```\n",
    "method should be a tuple containing all the inputs, and similarly the argument to the\n",
    "```compute_output_shape()``` method should be a tuple containing each input’s batch\n",
    "shape. To create a layer with multiple outputs, the ```call()``` method should return the\n",
    "list of outputs, and the ```compute_output_shape()``` should return the list of batch output \n",
    "shapes (one per output). \n",
    "\n",
    "For example, the following toy layer takes two inputs\n",
    "and returns three outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1] # should probably handle broadcasting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer may now be used like any other layer, but of course only using the functional \n",
    "and subclassing APIs, not the sequential API (which only accepts layers with\n",
    "one input and one output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom Layers with different behaviour during testing and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your layer needs to have a different behavior during training and during testing\n",
    "(e.g., if it uses ```Dropout``` or ```BatchNormalization``` layers), then you must add a train\n",
    "ing argument to the call() method and use this argument to decide what to do. For\n",
    "example, let’s create a layer that adds Gaussian noise during training (for regularization), \n",
    "but does nothing during testing (tf.keras actually has a layer that does the same thing: ```tf.keras.layers.GaussianNoise``` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
